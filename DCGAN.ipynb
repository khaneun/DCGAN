{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DCGAN\n",
    "\n",
    "본 Notebook은 Alec Radford et al. 의 논문 'Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks' 내용을 해석하여 이해하고, 이를 모듈화하는 것을 목적으로 합니다. \n",
    "\n",
    "*Learning reusable feature representations from large unlabeled datasets has been an area of active research. In the context of computer vision, one can leverage the prectically unlimited amount of unlabeled images and videos to learn good intermediate representations, which can then be used on a variety of supervised learning tasks such as image classifiacation. *\n",
    "\n",
    "*We propose that one way to build good image representations is by training Generative Adversarial Networks (GANs, Goodfellow et al, 2014), and later reusing parts of the generator and discriminator networks as feature extractors for supervised tasks. GANs provide an attractive alternative to maximum likelihood techniques. *\n",
    "\n",
    "Ian Goodfellow의 GAN이 발표되고 나서 다양한 연구 분야에서 GAN을 활용하기 시작한 이래로, 쓸만한 output을 얻기 위한 trainig은 notorious 라는 수식이가 자연스레 붙을 정도로, 실제 적용하는 데에는 Mode collapse 등의 불안정한 구조라는 문제가 내재되어 있다. [NIPS 2016 Tutorial](https://www.youtube.com/watch?v=AJVyzd0rqdc)\n",
    "\n",
    "*One can additionally argue that their learning process and the lack of a heuristic cost function (such as pixel-wise independent mean-square error) are attractive to representation learning. <span style=\"color:red\"> GANs have been known to be unstable to train, often resulting in generators that produce nonsensical outputs. </span> There has been very limited published research in tyring to understand and visualize what GANs learn, and the intermediate representations of multi-layer GANs. *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/icell/anaconda3/envs/hog/lib/python3.5/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "\n",
    "논문 中 4. Details of Adversarial training\n",
    "\n",
    "*We trained DCGANs on three datasets, Large-scale Scene Understanding (LSUN) (Yu et al. 2015), Imagenet-1k and a newly assembled Faces dataset. Details on the usage of each of these datasets are given below.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "논문 中 3. Approach and Model Architecture\n",
    "\n",
    "*..**Architecture guidelines for stable Deep Convolutional GANs.** *\n",
    "- *Replace any pooling layers with strided convolutions (discriminator) and <span style=\"color:blue\">fractional-strided</span> convolutions (generator)*\n",
    "- *Use batchnorm in both the generator and the discriminator.*\n",
    "- *Remove fully connected hidden layers for deeper architectures.*\n",
    "- *Use ReLU activation in generator for all layer except for the output, which uses Tanh.*\n",
    "- *Use LeakyReLU activation in the discriminator for all layers.*\n",
    "\n",
    "※ GAN 자료를 살펴보다보면, 종종 Deconvolutional이라는 용어가 쓰이는데 fractional-strided 가 정확한 용어라고 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters\n",
    "\n",
    "논문 中 4. Details of Adversarial training\n",
    "\n",
    "*No pre-processing was applied to training images besides scaling to the range of the tanh activation function [-1, 1]. All models were trained with mini-batch stochastic gradient descent (SGD) with a mini-batch size of 128. All weights were initialized from a zero-centered Normal distribution with standard deviation 0.02. In the LeakyReLU, the slope of the leak was set to 0.2 in all models. While previous GAN work has used momentum to accelerate training, we used the Adam optimizer (Kingma & Ba, 2014) with tuned hyperparameters. We found the suggested learning rate of 0.001, to be too high, using 0.0002 instead. Additionally, we found leaving the momentum term β1 at the suggested value of 0.9 resulted in training oscillation and instability while reducing it to 0.5 helped stabilize training.*\n",
    "\n",
    "...주요한 Parameter만 정리하면,\n",
    "\n",
    "- Use Tanh activation function in output of generator\n",
    "- Use SGD with batch size of 128\n",
    "- Weight Initializer ~ N(0, sqrt(0.02))\n",
    "- LeakyReLU's slope is 0.2\n",
    "- Use Adam Optimizer\n",
    "- learning late sets 0.001\n",
    "- β1 sets 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "size_of_batch = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](imgs/figure1.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generator(z):    \n",
    "    \n",
    "    with tf.variable_scope('generator') as scope:\n",
    "\n",
    "        output_dimension = 64\n",
    "        conv4_dimension = int(output_dimension/ 2) # 32\n",
    "        conv3_dimension = int(conv4_dimension / 2) # 16\n",
    "        conv2_dimension = int(conv3_dimension / 2) # 8\n",
    "        conv1_dimension = int(conv2_dimension / 2) # 4\n",
    "        projection_dimension = conv1_dimension\n",
    "\n",
    "        output_channel = 3\n",
    "        projection_channel = 1024\n",
    "        conv1_channel = projection_channel\n",
    "        conv2_channel = int(conv1_channel / 2) # 512\n",
    "        conv3_channel = int(conv2_channel / 2) # 256\n",
    "        conv4_channel = int(conv3_channel / 2) # 128\n",
    "\n",
    "        size_filter = 5\n",
    "\n",
    "        # Project and reshape Layer\n",
    "        hidden_layer = tf.reshape(z, [size_of_batch, projection_dimension, projection_dimension, projection_channel ])\n",
    "        hidden_layer = tf.nn.relu(hidden_layer)\n",
    "\n",
    "        # Convolusional Layer 1\n",
    "        output_shape1 = [size_of_batch, conv2_dimension, conv2_dimension, conv2_channel]\n",
    "        w_conv1 = tf.get_variable('w_conv1',\n",
    "                                  [size_filter, size_filter, output_shape1[-1], int(hidden_layer.get_shape()[-1])],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('b_conv1',\n",
    "                                  [output_shape1[-1]],\n",
    "                                  initializer=tf.constant_initializer(0.5))\n",
    "\n",
    "        conv1 = tf.nn.conv2d_transpose(hidden_layer, w_conv1, \n",
    "                                       output_shape=output_shape1, strides=[1, 2, 2, 1], padding='SAME') + b_conv1\n",
    "        conv1 = tf.layers.batch_normalization(inputs=conv1, center=True, scale=True, is_training=True, scope='g_bn1')\n",
    "        conv1 = tf.nn.relu(conv1)\n",
    "\n",
    "        # Convolusional Layer 2\n",
    "        output_shape2 = [size_of_batch, conv3_dimension, conv3_dimension, conv3_channel]\n",
    "        w_conv2 = tf.get_variable('w_conv2',\n",
    "                                  [size_filter, size_filter, output_shape2[-1], int(conv1.get_shape()[-1])],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('b_conv2',\n",
    "                                  [output_shape2[-1]],\n",
    "                                  initializer=tf.constant_initializer(0.5))\n",
    "\n",
    "        conv2 = tf.nn.conv2d_transpose(conv1, w_conv2, \n",
    "                                       output_shape=output_shape2, strides=[1, 2, 2, 1],padding='SAME') + b_conv2\n",
    "        conv2 = tf.layers.batch_normalization(inputs=conv2, center=True, scale=True, is_training=True, scope='g_bn2')\n",
    "        conv2 = tf.nn.relu(conv2)\n",
    "\n",
    "        # Convolusional Layer 3\n",
    "        output_shape3 = [size_of_batch, conv4_dimension, conv4_dimension, conv4_channel]\n",
    "        w_conv3 = tf.get_variable('w_conv3',\n",
    "                                  [size_filter, size_filter, output_shape3[-1], int(conv2.get_shape()[-1])],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv3 = tf.get_variable('b_conv3',\n",
    "                                  [output_shape3[-1]],\n",
    "                                  initializer=tf.constant_initializer(0.5))\n",
    "\n",
    "        conv3 = tf.nn.conv2d_transpose(conv2, w_conv3, \n",
    "                                       output_shape=output_shape3, strides=[1, 2, 2, 1],padding='SAME') + b_conv3\n",
    "        conv3 = tf.layers.batch_normalization(inputs=conv3, center=True, scale=True, is_training=True, scope='g_bn3')\n",
    "        conv3 = tf.nn.relu(conv3)\n",
    "\n",
    "        # Convolusional Layer 4\n",
    "        output_shape4 = [size_of_batch, conv4_dimension, conv4_dimension, conv4_channel]\n",
    "        w_conv4 = tf.get_variable('w_conv4',\n",
    "                                  [size_filter, size_filter, output_shape4[-1], int(conv3.get_shape()[-1])],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv4 = tf.get_variable('b_conv4',\n",
    "                                  [output_shape4[-1]],\n",
    "                                  initializer=tf.constant_initializer(0.5))\n",
    "\n",
    "        conv4 = tf.nn.conv2d_transpose(conv3, w_conv4, \n",
    "                                       output_shape=output_shape4, strides=[1,2,2,1],padding='SAME') + b_conv4\n",
    "        conv4 = tf.layers.batch_normalization(inputs=conv4, center=True, scale=True, is_training=True, scope='g_bn4')\n",
    "        conv4 = tf.nn.tanh(conv4)\n",
    "\n",
    "        return conv4\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "논문 中 Figure 1\n",
    "\n",
    "*DCGAN generator used for LSUN scene modeling. A **<span style=\"color:blue\">100 dimensional</span>** uniform distribution Z is projected to a samll spatial extent convolutional representation with many feature maps. A series of four fractionally-strided convolutions (in some recent paper, these are wrongly called devonvolutions) then convert this high level representation into a 64 X 64 pixel image. Notably, no fully connected or pooling layers are used*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "initial_demension=100\n",
    "Z = tf.placeholder(np.float32, [None, initial_demension])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discriminator\n",
    "\n",
    "논문 中 3. Approach and Model Architecture\n",
    "\n",
    "*...The first is the all convolutional net (Springenberg et al., 2014) which replaces **<span style=\"color:blue\">deterministic spatial pooling functions (such as maxpooling)</span>** with strided convolutions, allowing the network to learn its own spatial downsampling. We use this approach in our generator, allowing it to learn its own spatial upsampling, and discriminator.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pooling_function(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discriminator(x, input_channel):\n",
    "    with tf.variable_scope('discriminator') as scope:\n",
    "        size_filter = 5\n",
    "        \n",
    "        w_conv1 = tf.get_variable('w_conv1', [size_filter, size_filter, input_channel, 128],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv1 = tf.get_variable('b_conv1', [128], initializer=tf.constant_initializer(0.5))\n",
    "        conv1 = tf.nn.conv2d(x, w_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "        conv1 = tf.layers.batch_normalization(inputs=conv1, center=True, scale=True, is_training=True, scope='d_bn1')\n",
    "        conv1 = tf.nn.leaky_relu(conv1, alpha=0.2)\n",
    "        conv1 = pooling_function(conv1)\n",
    "\n",
    "        w_conv2 = tf.get_variable('w_conv2', [size_filter, size_filter, int(conv1.get_shape()[-1]), 256],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv2 = tf.get_variable('b_conv2', [256], initializer=tf.constant_initializer(0.5))\n",
    "        conv2 = tf.nn.conv2d(conv1, w_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2\n",
    "        conv2 = tf.layers.batch_normalization(inputs=conv2, center=True, scale=True, is_training=True, scope='d_bn2')\n",
    "        conv2 = tf.nn.leaky_relu(conv2, alpha=0.2)\n",
    "        conv2 = pooling_function(conv2)\n",
    "        \n",
    "        w_conv3 = tf.get_variable('w_conv3', [size_filter, size_filter, int(conv2.get_shape()[-1]), 512],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv3 = tf.get_variable('b_conv3', [512], initializer=tf.constant_initializer(0.5))\n",
    "        conv3 = tf.nn.conv2d(conv2, w_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3\n",
    "        conv3 = tf.layers.batch_normalization(inputs=conv3, center=True, scale=True, is_training=True, scope='d_bn3')\n",
    "        conv3 = tf.nn.leaky_relu(conv3, alpha=0.2)\n",
    "        conv3 = pooling_function(conv3)\n",
    "\n",
    "        w_conv4 = tf.get_variable('w_conv4', [size_filter, size_filter, int(conv3.get_shape()[-1]), 1],\n",
    "                                  initializer=tf.random_normal_initializer(mean=0, stddev=0.02))\n",
    "        b_conv4 = tf.get_variable('b_conv4', [1], initializer=tf.constant_initializer(0.5))\n",
    "        conv4 = tf.nn.conv2d(conv3, w_conv4, strides=[1, 1, 1, 1], padding='SAME') + b_conv4\n",
    "        conv4 = tf.layers.batch_normalization(inputs=conv4, center=True, scale=True, is_training=True, scope='d_bn1')\n",
    "        conv4 = tf.nn.leaky_relu(conv4, alpha=0.2)\n",
    "        \n",
    "        return conv4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
